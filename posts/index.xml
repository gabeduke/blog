<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Weblog on Adventures in over-engineering</title><link>https://gabeduke.github.io/weblog/posts/</link><description>Recent content in Weblog on Adventures in over-engineering</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 27 Feb 2020 09:30:08 -0500</lastBuildDate><atom:link href="https://gabeduke.github.io/weblog/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Faasd in LXC</title><link>https://gabeduke.github.io/weblog/posts/2020/02/faasd-in-lxc/</link><pubDate>Thu, 27 Feb 2020 09:30:08 -0500</pubDate><guid>https://gabeduke.github.io/weblog/posts/2020/02/faasd-in-lxc/</guid><description>TL/DR
This post explores running faasd in LXC containers.
As a consumer of Faasd on a linux workstation I wanted an options to run in native containers. Faasd Developers can benefit from a native runtime environment, as well as cheap/fast sandbox environments provided by LXC. OpenFaas function function developers can benefit from a tighter inner-loop that doesnt invole pushing docker images to a remote registry just to test their functions Introduction First lets discuss what exactly is faasd and what it can be used for.</description><content type="html"><![CDATA[<p><strong>TL/DR</strong></p>
<p>This post explores running <a href="https://github.com/openfaas/faasd">faasd</a> in LXC containers.</p>
<ul>
<li>As a consumer of Faasd on a linux workstation I wanted an options to run in native containers.</li>
<li>Faasd Developers can benefit from a native runtime environment, as well as cheap/fast sandbox environments provided by LXC.</li>
<li>OpenFaas function function developers can benefit from a tighter inner-loop that doesnt invole pushing docker images to a remote registry just to test their functions</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>First lets discuss what exactly is faasd and what it can be used for. Faasd is a provider implementation of OpenFaas run as a systemd daemon. There are a handful of providers that each plug into OpenFaas which enable it to run in different environments. For example there is a provider for docker, kubernetes and even an in-memory provider for an ephemeral experience. Faasd is really geared for low resource boards like Raspberry Pi, where the overhead of running a docker daemon is too high (faasd interfaces directly with containerd using runc). So what does this mean? Well faasd makes a nice controller-manager for very small containerized processes.</p>
<p>At the time of this post, faasd runs purely as systemd processes, the components are:</p>
<ul>
<li>Faasd/faas-cli - systemd daemon and go CLI for managing functions of http</li>
<li>Containerd/Runc - container runtime (all functions must be containerized)</li>
<li>CNI - I havn't dug too deep here but I assume this is for creating and cleaning up IP addresses as all functions must be network callable</li>
</ul>
<h3 id="why-lxc">Why LXC?</h3>
<p>Okay cool so it sounds like the functions are already in containers, why bring LXC into the mix? First let's take a look at how the components are deployed. The current set up is quite simple, just a few binaries and some systemd templates. These are all glued together in a simple <a href="https://cloudinit.readthedocs.io/en/latest/index.html">cloud-config</a>. This is awesome, basically our only requirement is a fresh linux OS that can execute the script.</p>
<p>But what if we want to run faasd on an existing linux system? Of course the simplest option, we can run a Virtual Machine, but we will have to pay the penalty of virtualization.</p>
<blockquote>
<p><em>confession: I do not actually know the overhead of running a VM these days. It's probable the host would never know the diffence since I understand it is quite low.</em></p>
</blockquote>
<p>Another option would be to run the processes directly on the host. This gets super messy without config managment tools like ansible. Given the current deployment is a handful of bootstrap scripts it would be a total pain to track things down if something breaks or we need to upgrade.</p>
<p>Which brings us to LXC. Linux Containers are simply namespaced filesystems running on top of a shared linux kernel. Some advantages to running faasd in LXC (beyond the above mentioned):</p>
<ol>
<li>We can easily spin up <code>n+</code> environments including different versions for testing</li>
<li>Containers are <em>extremely</em> fast to boot once the initial file system is pulled down.</li>
<li>When something breaks we can blow away the entire container with minimal risk to the host.</li>
<li>LXC actually has a network daemon included (LXD) which makes managing containers over the wire very convenient. See <a href="#Next_steps">Next steps</a> for a thought experiment involving LXD.</li>
</ol>
<h2 id="demo">Demo</h2>
<p>Okay enough yapping let's look at some code..</p>
<h3 id="pre-requisites">Pre-requisites</h3>
<p>You will need to <a href="https://linuxcontainers.org/lxd/getting-started-cli/">install LXD</a> and then run <code>lxd init</code> to set up the network and storage pool. I stuck with <code>dir</code> for the storage backend to keep things simple and <code>lxdbr0</code> for the network (bridge to the host).</p>
<blockquote>
<p><em>warning: don't use ZFS storage pool yet, containerd support is not quite there.</em></p>
</blockquote>
<h3 id="creating-a-container">Creating a Container</h3>
<p>The basic steps are:</p>
<ul>
<li>
<p>create a LXC profile with the faasd cloud-init script and <code>security.nesting</code> enabled (since we are using containerd inside of linux namespaces)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">LXD_PROFILE<span style="color:#f92672">=</span>faasd
LXD_USERDATA<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/tmp/cloud-config.yml&#34;</span>

wget -qO $LXD_USERDATA https://gist.githubusercontent.com/gabeduke/7ccb3f3147d79ac30e2187432808060c/raw/4028ea0658e9aa0a37f6ac44473a8092e3824406/cloud-init.yml
lxc profile set $LXD_PROFILE user.user-data - &lt; $LXD_USERDATA
lxc profile set $LXD_PROFILE security.nesting<span style="color:#f92672">=</span>true

lxc launch ubuntu:18.04 faasd -p $LXD_PROFILE

</code></pre></div></li>
<li>
<p>Once the container is finished bootstrapping we can fetch the faasd password and connect to the OPENFAAS_GATEWAY over the network</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">FAASD_PASSWORD<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>lxc exec faasd -- cat /var/lib/faasd/secrets/basic-auth-password<span style="color:#66d9ef">)</span>
FAASD_IP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>lxc exec faasd -- /sbin/ip -o -4 addr list eth0 | awk <span style="color:#e6db74">&#39;{print $4}&#39;</span> | cut -d/ -f1<span style="color:#66d9ef">)</span>

echo $FAASD_PASSWORD | faas-cli login --password-stdin --gateway http://<span style="color:#e6db74">${</span>FAASD_IP<span style="color:#e6db74">}</span>:8080

</code></pre></div></li>
<li>
<p>There should now be a container running:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">» lxc list
+-------+---------+-----------------------+----------------------------------------------+-----------+-----------+
| NAME  |  STATE  |         IPV4          |                     IPV6                     |   TYPE    | SNAPSHOTS |
+-------+---------+-----------------------+----------------------------------------------+-----------+-----------+
| faasd | RUNNING | 10.62.0.1 <span style="color:#f92672">(</span>openfaas0<span style="color:#f92672">)</span> | fd42:ad54:4bbd:9f9:216:3eff:fe50:66ee <span style="color:#f92672">(</span>eth0<span style="color:#f92672">)</span> | CONTAINER | <span style="color:#ae81ff">0</span>         |
|       |         | 10.225.76.18 <span style="color:#f92672">(</span>eth0<span style="color:#f92672">)</span>   |                                              |           |           |
+-------+---------+-----------------------+----------------------------------------------+-----------+-----------+

</code></pre></div></li>
<li>
<p>And OpenFaas should be addressable on the gateway IP:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">» faas store deploy figlet
WARNING! Communication is not secure, please consider using HTTPS. Letsencrypt.org offers free SSL/TLS certificates.

Deployed. <span style="color:#ae81ff">200</span> OK.
URL: http://faasd.local:8080/function/figlet

» faas list
Function                      	Invocations    	Replicas
figlet                        	<span style="color:#ae81ff">0</span>              	<span style="color:#ae81ff">1</span>    


</code></pre></div></li>
</ul>
<hr>
<p>Here is the full convenience script (I am using <a href="https://github.com/cbednarski/hostess">hostess</a> to update <code>/etc/hosts</code> with the new gateway IP):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash 
</span><span style="color:#75715e"></span>
<span style="color:#75715e"># Set up the environment</span>
export OPENFAAS_URL<span style="color:#f92672">=</span>http://faasd.local:8080

LXD_PROFILE<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>1<span style="color:#66d9ef">:-</span>dukemon<span style="color:#e6db74">}</span>
LXD_USERDATA<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/tmp/cloud-config.yml&#34;</span>
LXD_CONTAINER<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>2<span style="color:#66d9ef">:-</span>faasd<span style="color:#e6db74">}</span>

<span style="color:#66d9ef">if</span> ! <span style="color:#f92672">(</span>lxc info $LXD_CONTAINER &gt; /dev/null 2&gt;&amp;1<span style="color:#f92672">)</span>
<span style="color:#66d9ef">then</span>
	echo <span style="color:#e6db74">&#34;ensure lxc profile&#34;</span>
	wget -qO $LXD_USERDATA https://gist.githubusercontent.com/gabeduke/7ccb3f3147d79ac30e2187432808060c/raw/4028ea0658e9aa0a37f6ac44473a8092e3824406/cloud-init.yml
	lxc profile set $LXD_PROFILE user.user-data - &lt; $LXD_USERDATA
	lxc profile set $LXD_PROFILE security.nesting<span style="color:#f92672">=</span>true

	echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">launching LXC container </span><span style="color:#e6db74">${</span>LXD_CONTAINER<span style="color:#e6db74">}</span><span style="color:#e6db74">..</span><span style="color:#e6db74">&#34;</span>
	lxc launch ubuntu:18.04 $LXD_CONTAINER -p $LXD_PROFILE
<span style="color:#66d9ef">fi</span>

<span style="color:#75715e"># Wait for cloud-init sequence to finish before moving on to the configuration steps</span>
<span style="color:#66d9ef">until</span> <span style="color:#f92672">(</span>lxc exec $LXD_CONTAINER -- cat /var/lib/cloud/instance/boot-finished &gt; /dev/null 2&gt;&amp;1<span style="color:#f92672">)</span>
<span style="color:#66d9ef">do</span>
	echo <span style="color:#e6db74">&#34;Waiting for cloud-init complete signal..&#34;</span>
	sleep <span style="color:#ae81ff">5</span>
<span style="color:#66d9ef">done</span>

<span style="color:#75715e"># let&#39;s be safe and wait for the last thread to finish</span>
wait

echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">fetch faasd password from </span>$LXD_CONTAINER<span style="color:#e6db74">&#34;</span>
FAASD_PASSWORD<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>lxc exec $LXD_CONTAINER -- cat /var/lib/faasd/secrets/basic-auth-password<span style="color:#66d9ef">)</span>

FAASD_IP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>lxc exec $LXD_CONTAINER -- /sbin/ip -o -4 addr list eth0 | awk <span style="color:#e6db74">&#39;{print $4}&#39;</span> | cut -d/ -f1<span style="color:#66d9ef">)</span>
echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">faasd IP is </span>$FAASD_IP<span style="color:#e6db74">&#34;</span>

<span style="color:#66d9ef">if</span> which hostess
<span style="color:#66d9ef">then</span>
	echo <span style="color:#e6db74">&#34;Updating /etc/hosts&#34;</span>
	sudo hostess add <span style="color:#e6db74">&#34;faasd.local&#34;</span> $FAASD_IP

	wait

	<span style="color:#66d9ef">until</span> <span style="color:#f92672">(</span>echo $FAASD_PASSWORD | faas-cli login --password-stdin &gt; /dev/null 2&gt;&amp;<span style="color:#ae81ff">1</span> <span style="color:#f92672">&amp;&amp;</span> echo <span style="color:#e6db74">&#34;login successful&#34;</span><span style="color:#f92672">)</span>
	<span style="color:#66d9ef">do</span>
		echo <span style="color:#e6db74">&#34;Attempt faasd login..&#34;</span>
		sleep <span style="color:#ae81ff">5</span>
	<span style="color:#66d9ef">done</span>
<span style="color:#66d9ef">else</span>
       	echo <span style="color:#e6db74">&#34;install hostess to /usr/local/bin to automatically update /etc/hosts (https://github.com/cbednarski/hostess)&#34;</span>
<span style="color:#66d9ef">fi</span>

echo <span style="color:#e6db74">&#34;init complete!&#34;</span>

</code></pre></div><h3 id="hot-loading-images">Hot loading images</h3>
<p>This is great we now have a container running with all of the faasd components and can easily add copies or swap out a new version. There is one last issue which is shortening the development loop for buildling functions. Containerd does not re-pull an image so once we publish a tag we cannot update it. It is common in development cycles to use a tag like <code>latest</code> to keep publishing images to. But we are running the containerd socket locally so why would we even want to push to a remote registry just to pull the same image back down to a different location on our machine? We can do better..</p>
<p>Let's go over the steps to hot load images directly into containerd:</p>
<ul>
<li>
<p>When developing OpenFaas functions we can just build the images locally and save them to OCI compatiable images using docker</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">faas build -f fn.yml
docker save <span style="color:#f92672">[</span>registry<span style="color:#f92672">]</span>/<span style="color:#f92672">[</span>image<span style="color:#f92672">]</span>:<span style="color:#f92672">[</span>tag<span style="color:#f92672">]</span> -o fn.tar

</code></pre></div></li>
<li>
<p>Now we can push the tarball into the LXC container and import it to the correct namespace:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">lxc file push fn.tar faasd/
lxc exec faasd -- ctr -n openfaas-fn images import /fn.tar
  
</code></pre></div></li>
<li>
<p>Now we can simply run <code>faas deploy</code> to restart the container with the updated image.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">faasd deploy

</code></pre></div></li>
</ul>
<h2 id="next-steps">Next steps</h2>
<p>I had a lot of fun playing around with and containerizing Faasd, on a closing note here are some more things to try:</p>
<ul>
<li>Use LXD to create an autoscaling group of containers (need to ship container metrics and scale on capacity events)</li>
<li>Use <code>distrobuilder</code> to pack a sharable image for more of a docker or ISO like experience.</li>
</ul>
]]></content></item><item><title>Banzai Kafka in Kubernetes</title><link>https://gabeduke.github.io/weblog/posts/2020/01/banzai-kafka-in-kubernetes/</link><pubDate>Thu, 09 Jan 2020 09:30:08 -0500</pubDate><guid>https://gabeduke.github.io/weblog/posts/2020/01/banzai-kafka-in-kubernetes/</guid><description>This example deploys a Kafka cluster using Banzai Cloud Kafka Operator. This guide is a condensed version of the official Banzai documentation, the idea being to quickly get up and running with Kafka.
Setup Prerequisites:
Civo CLI (Not strictly necessary but it is FAST) Helm3 Kubectl Provision Kubernetes Any kubernetes cluster will do but for this tutorial I'll use Civo Cloud's offering.
civo k8s create \ --nodes 3 \ --save --switch --wait \ kafka Provision Dependencies Let's get the core dependencies provisioned.</description><content type="html"><![CDATA[<p>This example deploys a Kafka cluster using <a href="https://github.com/banzaicloud/kafka-operator">Banzai Cloud Kafka Operator</a>. This guide is a condensed version of the official Banzai documentation, the idea being to quickly get up and running with Kafka.</p>
<h2 id="setup">Setup</h2>
<p><em>Prerequisites</em>:</p>
<ul>
<li><strong><a href="https://github.com/civo/cli">Civo CLI</a></strong> (Not strictly necessary but it is <em>FAST</em>)</li>
<li><strong><a href="https://helm.sh/docs/intro/install/">Helm3</a></strong></li>
<li><strong><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Kubectl</a></strong></li>
</ul>
<h3 id="provision-kubernetes">Provision Kubernetes</h3>
<p>Any kubernetes cluster will do but for this tutorial I'll use <a href="https://www.civo.com/">Civo Cloud's</a> offering.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">civo k8s create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --nodes <span style="color:#ae81ff">3</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --save --switch --wait <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  kafka
</code></pre></div><h3 id="provision-dependencies">Provision Dependencies</h3>
<p>Let's get the core dependencies provisioned. We'll need:</p>
<ul>
<li><strong>Cert Manager</strong>: cert manager off loads the heavy lifting for certificate generation and rotation. There are many configurations that can be enabled, just check out the Banzai documentation for more information.</li>
<li><strong>Prometheus Operator</strong>: We only need the core bundle which will enable the use of <a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md#related-resources">service monitors</a> and alerts. This is one of the key enablers that allows Banzai kafka clusters to recover and rebalance data.</li>
<li><strong>Zookeeper</strong>: You can use any zookeeper endpoint but Banzai has packaged an operator for use. Zookeeper is the key value database which stores the kafka state.</li>
</ul>
<p><strong>Note</strong>: This tutorial should be fully recreatable from the clodeblocks. However, it may be helpful to wait until each workload finishes deploying before moving on to the next command. I recommend putting a watch on the cluster pods in a separate terminal <code>watch kubectl get pods --all-namespaces</code>.</p>
<p>We'll also need the <em>Banzai Helm Repo</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add banzaicloud-stable https://kubernetes-charts.banzaicloud.com/
</code></pre></div><p><em>Cert-Manager</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create cert-manager deployment</span>
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.11.0/cert-manager.yaml
</code></pre></div><p><em>Prometheus Operator</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create the prometheus operator</span>
kubectl apply -n default -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/bundle.yaml
</code></pre></div><p><em>Zookeeper Operator</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create zookeeper namespace</span>
kubectl create namespace zookeeper

<span style="color:#75715e"># create zk operator</span>
helm upgrade --install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --namespace zookeeper <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --wait <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  zookeeper-operator banzaicloud-stable/zookeeper-operator
</code></pre></div><p><em>Zookeeper</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create zookeeper cluster</span>
cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply --namespace zookeeper -f -
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">apiVersion: zookeeper.pravega.io/v1beta1
</span><span style="color:#e6db74">kind: ZookeeperCluster
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: zookeepercluster
</span><span style="color:#e6db74">  namespace: zookeeper
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  replicas: 3
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><h3 id="provision-banzai-kafka">Provision Banzai Kafka</h3>
<p>Again there are lots of configurations details this guide does not cover. Please see the <a href="https://github.com/banzaicloud/kafka-operator">Banzai documentation</a> for more options.</p>
<p>Components:</p>
<ul>
<li><strong>Kafka Operator</strong>: will maintain the lifecycle, data rebalancing and scaling for all the provisioned Kafkas in the cluster.</li>
<li><strong>Kafka Instance</strong>: this guide provisions a simple kafka instance, configured for internal cluster access and initialized with some basic scaling/rebalancing rules.</li>
</ul>
<p><em>Kafka Operator</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create the kafka namespace</span>
kubectl create namespace kafka

<span style="color:#75715e"># get the values file configured for prometheus</span>
TMP_FILE<span style="color:#f92672">=</span>/tmp/kafka-prometheus-alerts.yaml

curl https://raw.githubusercontent.com/gabeduke/civo-kafka-example/v1.0.0/kafka-prometheus-alerts.yaml -o $TMP_FILE -s

<span style="color:#75715e"># install kafka operator with prometheus alerts</span>
helm upgrade --install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --namespace kafka <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --values $TMP_FILE <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  kafka-operator banzaicloud-stable/kafka-operator
</code></pre></div><p><em>Kafka</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create the kafka cluster</span>
KAFKA_INSTANCE<span style="color:#f92672">=</span>https://raw.githubusercontent.com/gabeduke/civo-kafka-example/v1.0.0/kafka.yaml
curl $KAFKA_INSTANCE | kubectl apply -n kafka -f -

<span style="color:#75715e"># create the service monitor</span>

KAFKA_SERVICE_MONITOR<span style="color:#f92672">=</span>https://raw.githubusercontent.com/gabeduke/civo-kafka-example/v1.0.0/kafka-prometheus.yaml
curl $KAFKA_SERVICE_MONITOR | kubectl apply -n kafka -f -
</code></pre></div><h3 id="validate">Validate</h3>
<p>First we will validate the Cruise Control Dashboard is online and healthy. <a href="https://github.com/linkedin/cruise-control">Cruise Control</a> is a tool from LinkedIn which provides exceptional operational control over kafka clusters. The API can be triggered via Prometheus alerts, making Banzai clusters highly resilient. Go ahead and explore the configuration options.</p>
<p><em>Cruise Control Dashboard</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># proxy to the cruise-control dashboard for kafka maintenance (may take a couple minutes)</span>
kubectl port-forward -n kafka svc/kafka-cruisecontrol-svc 8090:8090 &amp;
echo http://localhost:8090
</code></pre></div><p>We can also validate that we can produce and consume from this cluster. First we need to provision a topic to use (<em>note</em>: the cluster must be finished provisioning before the topic can be applied):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create a topic to which we can produce/consume</span>
cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">apiVersion: kafka.banzaicloud.io/v1alpha1
</span><span style="color:#e6db74">kind: KafkaTopic
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: civo-topic
</span><span style="color:#e6db74">  namespace: kafka
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  clusterRef:
</span><span style="color:#e6db74">    name: kafka
</span><span style="color:#e6db74">  name: civo-topic
</span><span style="color:#e6db74">  partitions: 3
</span><span style="color:#e6db74">  replicationFactor: 2
</span><span style="color:#e6db74">  config:
</span><span style="color:#e6db74">    &#34;retention.ms&#34;: &#34;604800000&#34;
</span><span style="color:#e6db74">    &#34;cleanup.policy&#34;: &#34;delete&#34;
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>Run the following two commands in separate terminals:</p>
<p><em>Produce</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># run a producer in a pod</span>
kubectl run kafka-producer <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  -n kafka -it --rm<span style="color:#f92672">=</span>true --restart<span style="color:#f92672">=</span>Never <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --image<span style="color:#f92672">=</span>wurstmeister/kafka:2.12-2.3.0 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -- /opt/kafka/bin/kafka-console-producer.sh <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --broker-list kafka-headless:29092 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --topic civo-topic
</code></pre></div><p><em>Consume</em>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># run a consumer in a pod</span>
kubectl run kafka-consumer <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  -n kafka -it --rm<span style="color:#f92672">=</span>true --restart<span style="color:#f92672">=</span>Never <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --image<span style="color:#f92672">=</span>wurstmeister/kafka:2.12-2.3.0 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -- /opt/kafka/bin/kafka-console-consumer.sh <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --bootstrap-server kafka-headless:29092 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --from-beginning <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --topic civo-topic
</code></pre></div><h2 id="clean">Clean</h2>
<p>Well that was fun. Go check out the Banzai docs and tune a cluster to your needs. Time to clean up!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># kill any dangling proxies</span>
killall kubectl

<span style="color:#75715e"># clean up the kubernetes cluster</span>
civo k8s delete kafka
</code></pre></div>]]></content></item><item><title>IOT Fleet Metrics (Part1)</title><link>https://gabeduke.github.io/weblog/posts/2019/11/iot-fleet-metrics-part1/</link><pubDate>Tue, 19 Nov 2019 09:30:04 -0500</pubDate><guid>https://gabeduke.github.io/weblog/posts/2019/11/iot-fleet-metrics-part1/</guid><description>TL/DR This guide documents a simple Prometheus PushGateway setup on top of Civo's k3s offering. We will then push some data to the gateway and visualize it in Grafana.
The end result for this project is an environmental monitoring system that gathers sensor data. I won't actually deploy the scrape jobs in this guide, but we will send a metric with curl and visualize it in each of the core components.</description><content type="html"><![CDATA[<h2 id="tldr">TL/DR</h2>
<p>This guide documents a simple Prometheus PushGateway setup on top of <a href="https://www.civo.com/kube100">Civo's k3s</a> offering. We will then push some data to the gateway and visualize it in Grafana.</p>
<p>The end result for this project is an environmental monitoring system that gathers sensor data. I won't actually deploy the scrape jobs in this guide, but we will send a metric with curl and visualize it in each of the core components. The subsequent blogs will document building a native kubernetes operator to manage the sensor inputs.</p>
<p><img src="/project.png" alt="Civo IOT Design"></p>
<h2 id="table-of-contents">Table of Contents</h2>
<!-- raw HTML omitted -->
<ul>
<li><a href="#tldr">TL/DR</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#pre-requisites">Pre-requisites</a>
<ul>
<li><a href="#tools">Tools</a></li>
<li><a href="#setup">Setup</a></li>
</ul>
</li>
<li><a href="#provision-cluster">Provision Cluster</a></li>
<li><a href="#deploy-core-applications">Deploy Core Applications</a>
<ul>
<li><a href="#install-grafana">Install Grafana</a></li>
<li><a href="#install-prometheus">Install Prometheus</a></li>
<li><a href="#install-push-gateway">Install Push-Gateway</a></li>
</ul>
</li>
<li><a href="#visualize-data">Visualize data</a>
<ul>
<li><a href="#visualize-in-pushgateway">Visualize in PushGateway</a></li>
<li><a href="#visualize-in-prometheus">Visualize in Prometheus</a></li>
<li><a href="#visualize-in-grafana">Visualize in Grafana</a></li>
</ul>
</li>
<li><a href="#wrapping-up">Wrapping up</a></li>
</ul>
<!-- raw HTML omitted -->
<h2 id="summary">Summary</h2>
<p>Prometheus PushGateway is a very useful tool to visualize batch metrics. This guide will walk through setting up a pushgateway instance and logging metrics from a simple BASH script. This is a great way to quickly visualize data <em>external</em> to kubernetes in a dead simple, sysadmin friendly way. The actual purpose of this project is to pull IOT sensor data into Prometheus, but I have tried to keep things as generic as possible in this first guide. In the next post I will begin building out the data logging functions into a kubernetes native operator.</p>
<p>The stack is deployed to <a href="https://www.civo.com/">Civo</a> cloud which runs a slimmed down flavor of kubernetes, called <a href="https://github.com/rancher/k3s">k3s</a>.</p>
<p>Components:</p>
<ul>
<li>K3s Cluster installed through Civo Cloud</li>
<li>Prometheus Operator (installed alongside the cluster via Civo marketplace)</li>
<li>PushGateway (installed via Helm)</li>
<li>Grafana (installed via Helm)</li>
</ul>
<h2 id="pre-requisites">Pre-requisites</h2>
<h3 id="tools">Tools</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/civo/cli#set-up">Civo-CLI</a></td>
<td>v0.5.1</td>
</tr>
<tr>
<td><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Kubectl</a></td>
<td>v1.16.3</td>
</tr>
</tbody>
</table>
<p><strong>Notes</strong>:</p>
<ul>
<li><a href="https://kustomize.io/">Kustomize</a>: is part of the kubectl binary since v1.15. Generally helm is used for doing the heavy lifting creating an application, and kustomize steps in for the lighter pieces that require last minute transforms.</li>
<li>K3s ships with a helm operator that takes a CRD which can be applied with <code>kubectl</code>. This guide will deploy grafana and pushgateway using this method. More information can be found <a href="https://rancher.com/docs/k3s/latest/en/configuration/#auto-deploying-manifests">here</a></li>
</ul>
<h3 id="setup">Setup</h3>
<p>Before getting started let's export some variables so they will be available throughout this guide. We also want to update our helm repo with the latest charts:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export CLUSTER_NAME<span style="color:#f92672">=</span>civo-iot-<span style="color:#66d9ef">$(</span>whoami<span style="color:#66d9ef">)</span>
export NAMESPACE<span style="color:#f92672">=</span>default
</code></pre></div><h2 id="provision-cluster">Provision Cluster</h2>
<p>The first step is to provision a K3s cluster using the <a href="https://www.civo.com/learn/kubernetes-cluster-administration-using-civo-cli">Civo CLI</a>.</p>
<p>This will take a couple minutes, once finished the <code>--save</code> flag wil point your kubectl context to the new cluster.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">civo kubernetes create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --applications prometheus-operator <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --nodes <span style="color:#ae81ff">2</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --save --switch --wait <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    <span style="color:#e6db74">${</span>CLUSTER_NAME<span style="color:#e6db74">}</span>
</code></pre></div><p>We are initializing the cluster with the <code>prometheus-operator</code> application from the <a href="https://github.com/civo/kubernetes-marketplace">Civo Marketplace</a>. Once the cluster has finished booting you can explore the default cluster monitors, provisioned by the prometheus operator. First port-forward to the grafana instance: <code>kubectl port-forward svc/prometheus-operator-grafana 8080:80 --namespace monitoring</code> and navigate to <code>http://localhost:8080</code> . You can log in with the username <code>admin</code> and the password <code>prom-operator</code>. Not every dashboard will work since the k3s distribution has a slightly different topology then a vanilla kubernetes cluster.</p>
<p>In the next steps we will provision our own instances of Prometheus and Grafana.</p>
<h2 id="deploy-core-applications">Deploy Core Applications</h2>
<p>The stack consists of a few core applications, and jobs to fetch the data.</p>
<ul>
<li><strong>Grafana</strong>: is a powerful visualization tool we will use for displaying our metrics. This could be considered the &lsquo;frontend&rsquo; of our application.</li>
<li><strong>Prometheus</strong>: is a time-series database that scales incredibly well. This is our &lsquo;backend&rsquo;. Prometheus is generally configured to scrape metrics data from applications on regular intervals.</li>
<li><strong>PushGateway</strong>: is a &lsquo;sink&rsquo; or &lsquo;buffer&rsquo; for metric data that is too short lived for Prometheus to scrape. This is what our cron jobs will log data to since the containers wont live long enough for Prometheus to ever see them.</li>
</ul>
<h3 id="install-grafana">Install Grafana</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
<span style="color:#75715e"># deploy/charts/grafana.yaml</span>
#
cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /tmp/grafana.yaml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChart
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: grafana
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  chart: stable/grafana
</span><span style="color:#e6db74">  version: 4.0.4
</span><span style="color:#e6db74">  targetNamespace: default
</span><span style="color:#e6db74">  valuesContent: |-
</span><span style="color:#e6db74">    datasources:
</span><span style="color:#e6db74">      datasources.yaml:
</span><span style="color:#e6db74">        apiVersion: 1
</span><span style="color:#e6db74">        datasources:
</span><span style="color:#e6db74">        - name: Prometheus
</span><span style="color:#e6db74">          type: prometheus
</span><span style="color:#e6db74">          url: http://prometheus-operated:9090
</span><span style="color:#e6db74">          access: proxy
</span><span style="color:#e6db74">          isDefault: true
</span><span style="color:#e6db74">EOF</span>

<span style="color:#75715e"># Apply the chart</span>
kubectl apply -f /tmp/grafana.yaml
</code></pre></div><h3 id="install-prometheus">Install Prometheus</h3>
<p>When we provisioned the cluster we installed the prometheus operator which installs an instance of prometheus by default. This instance is used for monitoring the cluster so we generally want to avoid using it for application data. Luckily operators make it super easy to spawn new instances. We simply need to create a Prometheus CRD and attach some RBAC permissions.</p>
<p>This is what the directory tree looks like:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">deploy/manifest/prometheus
├── kustomization.yaml
├── prometheus-rolebinding.yaml
├── prometheus-role.yaml
├── prometheus-sa.yaml
└── prometheus.yaml
</code></pre></div><p>To install we can build the directory with <code>kustomize</code> and pipe it directly to the cluster:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">TARGET<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;github.com/gabeduke/civo-iot/deploy/manifest/prometheus/?ref=1.0.0&#34;</span>

<span style="color:#75715e"># # If you have the repository checked out then you can uncomment the following line</span>
<span style="color:#75715e"># TARGET=deploy/manifest/prometheus</span>

kubectl kustomize <span style="color:#e6db74">${</span>TARGET<span style="color:#e6db74">}</span> | kubectl apply -n <span style="color:#e6db74">${</span>NAMESPACE<span style="color:#e6db74">}</span> -f -

</code></pre></div><h3 id="install-push-gateway">Install Push-Gateway</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
<span style="color:#75715e"># deploy/charts/pushgateway.yaml</span>
#
cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /tmp/pushgateway.yaml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChart
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: metrics-sink
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  chart: stable/prometheus-pushgateway
</span><span style="color:#e6db74">  version: 1.2.5
</span><span style="color:#e6db74">  targetNamespace: default
</span><span style="color:#e6db74">  set:
</span><span style="color:#e6db74">    metrics.enabled: &#34;true&#34;
</span><span style="color:#e6db74">    serviceMonitor.enabled: &#34;true&#34;
</span><span style="color:#e6db74">    serviceMonitor.namespace: &#34;default&#34;
</span><span style="color:#e6db74">EOF</span>

<span style="color:#75715e"># Apply the chart</span>
kubectl apply -f /tmp/pushgateway.yaml
</code></pre></div><h2 id="visualize-data">Visualize data</h2>
<p>Lets validate that the services are all working by pushing a data point to PushGateway manually.</p>
<p>Wait until alll pods are running and then start the proxies:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
<span style="color:#75715e"># Proxy Grafana</span>
kubectl port-forward svc/grafana -n <span style="color:#e6db74">${</span>NAMESPACE<span style="color:#e6db74">}</span> 8080:80 &amp;

<span style="color:#75715e"># Proxy Prometheus</span>
kubectl port-forward svc/prometheus-operated -n <span style="color:#e6db74">${</span>NAMESPACE<span style="color:#e6db74">}</span> 9090:9090 &amp;

<span style="color:#75715e"># Proxy PushGateway</span>
kubectl port-forward svc/metrics-sink-prometheus-pushgateway -n <span style="color:#e6db74">${</span>NAMESPACE<span style="color:#e6db74">}</span> 9091:9091 &amp;
</code></pre></div><p>Once the proxies are active you can drop a metric onto the pushgateway:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;sample_metric 1&#34;</span> | curl --silent --data-binary @- <span style="color:#e6db74">&#34;http://localhost:9091/metrics/job/sanity-test&#34;</span>
</code></pre></div><h3 id="visualize-in-pushgateway">Visualize in PushGateway</h3>
<p>http://localhost:9091</p>
<p>Notice there is a new group for <code>sanity-test</code> and the data point <code>sample_metric</code> is equal to 1.</p>
<p><img src="/pushgateway.png" alt=""></p>
<p>To see the raw metrics that prometheus will scrape, navigate to http://localhost:9091/metrics and notice the new line:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># TYPE sample_metric untyped</span>
sample_metric<span style="color:#f92672">{</span>instance<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>,job<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sanity-test&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">1</span>
</code></pre></div><h3 id="visualize-in-prometheus">Visualize in Prometheus</h3>
<p>http://localhost:9090</p>
<p>Prometheus is where the data will be aggregated and we can perform queries over time. Since we only have a single data point we will see a line in the graph when searching for <code>sample_metric</code>. As we build out the monitoring system we can add CRDs to generate alerts on our data.</p>
<p><img src="/prometheus.png" alt=""></p>
<h3 id="visualize-in-grafana">Visualize in Grafana</h3>
<p>Grafana is where we will compile dashboards to display Prometheus queries.</p>
<p>To get the password from the next step: <code>kubectl get secret --namespace default grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo</code></p>
<p>Log in with the username <code>admin</code> and password will be <code>${PASSWORD}</code>. Again the visualization is not very interesting with a single data point but this is a simple sanity test.</p>
<p>To validate our sample metric we are going to use the <em>Explore</em> function. Navigate to http://localhost:8080/explore</p>
<p><img src="/grafana_explore.png" alt=""></p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>Congradulations! You now have the foundation for a batch metrics monitoring system! Keep an eye out for the next post where I will walk thorugh connecting real sensor data.</p>
]]></content></item></channel></rss>